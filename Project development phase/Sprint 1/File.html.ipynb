{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CPHhAD660VuY"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "train_datagen = ImageDataGenerator(rescale=1./255,zoom_range=0.2,horizontal_flip=True,shear_range=0.2)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "x_train=train_datagen.flow_from_directory(\"/content/drive/MyDrive/dataset/train_set\",target_size=(64,64),class_mode='categorical',batch_size=5,color_mode='rgb')\n",
        "x_test=test_datagen.flow_from_directory(r\"/content/drive/MyDrive/dataset/test_set\",target_size=(64,64),class_mode='categorical',batch_size=5,color_mode='rgb')\n",
        "\n",
        "Found 742 images belonging to 4 classes.\n",
        "Found 198 images belonging to 4 classes.\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense,Conv2D,MaxPooling2D,Flatten\n",
        "\n",
        "model=Sequential()\n",
        "model.add(Conv2D(32,(3,3),input_shape=(64,64,3),activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Conv2D(32,(3,3),activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(units=128,activation='relu'))\n",
        "model.add(Dense(units=4,activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "Model: \"sequential_1\"\n",
        "_________________________________________________________________\n",
        " Layer (type)                Output Shape              Param #   \n",
        "=================================================================\n",
        " conv2d_2 (Conv2D)           (None, 62, 62, 32)        896       \n",
        "                                                                 \n",
        " max_pooling2d_2 (MaxPooling  (None, 31, 31, 32)       0         \n",
        " 2D)                                                             \n",
        "                                                                 \n",
        " conv2d_3 (Conv2D)           (None, 29, 29, 32)        9248      \n",
        "                                                                 \n",
        " max_pooling2d_3 (MaxPooling  (None, 14, 14, 32)       0         \n",
        " 2D)                                                             \n",
        "                                                                 \n",
        " flatten_1 (Flatten)         (None, 6272)              0         \n",
        "                                                                 \n",
        " dense_2 (Dense)             (None, 128)               802944    \n",
        "                                                                 \n",
        " dense_3 (Dense)             (None, 4)                 516       \n",
        "                                                                 \n",
        "=================================================================\n",
        "Total params: 813,604\n",
        "Trainable params: 813,604\n",
        "Non-trainable params: 0\n",
        "_________________________________________________________________\n",
        "\n",
        "model.fit_generator(generator=x_train,steps_per_epoch=len(x_train),validation_data=x_test,validation_steps=len(x_test),epochs=20)\n",
        "\n",
        "Epoch 1/20\n",
        "\n",
        "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
        "  \"\"\"Entry point for launching an IPython kernel.\n",
        "\n",
        "149/149 [==============================] - 44s 293ms/step - loss: 1.1635 - accuracy: 0.4798 - val_loss: 0.9364 - val_accuracy: 0.6566\n",
        "Epoch 2/20\n",
        "149/149 [==============================] - 41s 273ms/step - loss: 0.8416 - accuracy: 0.6429 - val_loss: 0.8283 - val_accuracy: 0.6717\n",
        "Epoch 3/20\n",
        "149/149 [==============================] - 42s 284ms/step - loss: 0.6678 - accuracy: 0.7655 - val_loss: 0.7795 - val_accuracy: 0.7323\n",
        "Epoch 4/20\n",
        "149/149 [==============================] - 41s 273ms/step - loss: 0.6775 - accuracy: 0.7493 - val_loss: 0.6493 - val_accuracy: 0.7626\n",
        "Epoch 5/20\n",
        "149/149 [==============================] - 41s 273ms/step - loss: 0.5995 - accuracy: 0.7749 - val_loss: 0.6781 - val_accuracy: 0.7879\n",
        "Epoch 6/20\n",
        "149/149 [==============================] - 41s 273ms/step - loss: 0.5397 - accuracy: 0.7817 - val_loss: 0.8131 - val_accuracy: 0.7172\n",
        "Epoch 7/20\n",
        "149/149 [==============================] - 42s 285ms/step - loss: 0.4696 - accuracy: 0.8275 - val_loss: 0.6780 - val_accuracy: 0.7879\n",
        "Epoch 8/20\n",
        "149/149 [==============================] - 41s 272ms/step - loss: 0.4959 - accuracy: 0.8194 - val_loss: 0.8018 - val_accuracy: 0.7576\n",
        "Epoch 9/20\n",
        "149/149 [==============================] - 41s 273ms/step - loss: 0.3969 - accuracy: 0.8544 - val_loss: 0.6865 - val_accuracy: 0.7828\n",
        "Epoch 10/20\n",
        "149/149 [==============================] - 41s 273ms/step - loss: 0.3885 - accuracy: 0.8652 - val_loss: 0.8218 - val_accuracy: 0.7677\n",
        "Epoch 11/20\n",
        "149/149 [==============================] - 42s 280ms/step - loss: 0.3552 - accuracy: 0.8652 - val_loss: 1.0350 - val_accuracy: 0.7374\n",
        "Epoch 12/20\n",
        "149/149 [==============================] - 41s 273ms/step - loss: 0.3266 - accuracy: 0.8801 - val_loss: 0.7144 - val_accuracy: 0.7778\n",
        "Epoch 13/20\n",
        "149/149 [==============================] - 40s 268ms/step - loss: 0.2738 - accuracy: 0.8949 - val_loss: 0.6965 - val_accuracy: 0.7879\n",
        "Epoch 14/20\n",
        "149/149 [==============================] - 40s 271ms/step - loss: 0.2957 - accuracy: 0.8827 - val_loss: 0.7882 - val_accuracy: 0.7677\n",
        "Epoch 15/20\n",
        "149/149 [==============================] - 42s 283ms/step - loss: 0.2576 - accuracy: 0.9084 - val_loss: 1.0848 - val_accuracy: 0.7525\n",
        "Epoch 16/20\n",
        "149/149 [==============================] - 41s 271ms/step - loss: 0.2901 - accuracy: 0.8976 - val_loss: 0.8777 - val_accuracy: 0.7828\n",
        "Epoch 17/20\n",
        "149/149 [==============================] - 41s 271ms/step - loss: 0.2853 - accuracy: 0.9097 - val_loss: 1.1820 - val_accuracy: 0.7273\n",
        "Epoch 18/20\n",
        "149/149 [==============================] - 42s 276ms/step - loss: 0.2219 - accuracy: 0.9272 - val_loss: 0.8731 - val_accuracy: 0.7929\n",
        "Epoch 19/20\n",
        "149/149 [==============================] - 41s 275ms/step - loss: 0.1894 - accuracy: 0.9353 - val_loss: 1.0621 - val_accuracy: 0.7374\n",
        "Epoch 20/20\n",
        "149/149 [==============================] - 40s 269ms/step - loss: 0.2297 - accuracy: 0.9151 - val_loss: 1.1963 - val_accuracy: 0.7374\n",
        "\n",
        "model.save('analysis.h5')\n",
        "model_json=model.to_json()\n",
        "with open(\"model-bw.json\",\"w\") as json_file:\n",
        "  json_file.write(model_json)\n",
        "\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing import image\n",
        "model=load_model('analysis.h5')\n",
        "\n",
        "x_train.class_indices\n",
        "\n",
        "{'Cyclone': 0, 'Earthquake': 1, 'Flood': 2, 'Wildfire': 3}\n",
        "\n",
        "img = image.load_img(r\"/content/drive/MyDrive/dataset/test_set/Earthquake/1347.jpg\",target_size=(64,64))\n",
        "x=image.img_to_array(img)\n",
        "x=np.expand_dims(x,axis=0)\n",
        "index=['Cyclone','Earthquake','Flood','Wildfire']\n",
        "y=np.argmax(model.predict(x),axis=1)\n",
        "print(index[int(y)])\n",
        "\n",
        "1/1 [==============================] - 0s 82ms/step\n",
        "Earthquake\n",
        "\n",
        "img = image.load_img(r\"/content/drive/MyDrive/dataset/test_set/Cyclone/918.jpg\",target_size=(64,64))\n",
        "x=image.img_to_array(img)\n",
        "x=np.expand_dims(x,axis=0)\n",
        "index=['Cyclone','Earthquake','Flood','Wildfire']\n",
        "y=np.argmax(model.predict(x),axis=1)\n",
        "print(index[int(y)])\n",
        "\n",
        "1/1 [==============================] - 0s 23ms/step\n",
        "Cyclone"
      ]
    }
  ]
}